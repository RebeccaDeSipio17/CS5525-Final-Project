Sale <- select(Sales)
library(dplyr)
summary(Carseats)
Sale <- select(Sales)
Sale <- Carseats(select(Sales))
sales <- Sales
sales
# split data into training set and test set
train=sample(1:nrow(Sales), 200)
Sales=Sales[-train,]
length(Carseats)
nrow(Carseats)
nrow(Sales)
nrow(Carseats(Sales))
nrow(sales)
?nrow
Sales
Carseats
# Load in libraries
library(glmnet)
# Load in the data
dat <- readRDS("Scheetz2006.rds")
attach(dat)
dim(dat$X)
# split data into training set and test set
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
High=ifelse(Sales<=8,"No","Yes")
High=as.factor(ifelse(Sales<=8,"No","Yes"))
# split data into training set and test set
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
# Fit a regression tree and plot the results
tree.car=tree(Sales~.,Carseats,subset=train)
summary(tree.car)
plot(tree.car, type = "unif")
text(tree.car,pretty=0,cex=0.7)
# obtain test MSE
pred=predict(tree.car,newdata=Carseats[-train,])
car.test=Carseats[-train,"Sales"]
(test.MSE <- mean((pred-car.test)^2))
## Problem 3
# Use cross validation to determine the optimal level of tree complexity
cv.car=cv.tree(tree.car)
cv.car
plot(cv.car)
# Prune the tree
prune.car=prune.tree(tree.car,best=9)
plot(prune.car,type = "unif")
text(prune.car,pretty=0,cex=0.7)
# Recalculate test MSE
# obtain test MSE
pred2=predict(tree.car,newdata=Carseats[-train,])
car.test2=Carseats[-train,"Sales"]
(test.MSE <- mean((pred2-car.test2)^2))
# Prune the tree
prune.car=prune.tree(tree.car,best=9)
plot(prune.car,type = "unif")
text(prune.car,pretty=0,cex=0.7)
## Problem 3
# Use cross validation to determine the optimal level of tree complexity
cv.car=cv.tree(tree.car)
cv.car
plot(cv.car)
min(cv.car$dev)
which.min(cv.car$dev)
(size(13))
(size[13])
(cv.car$size[13])
## Problem 3
# Use cross validation to determine the optimal level of tree complexity
cv.car=cv.tree(tree.car)
cv.car
plot(cv.car)
which.min(cv.car$dev)
best_val <- (cv.car$size[13])
# Prune the tree
prune.car=prune.tree(tree.car,best=best_val)
plot(prune.car,type = "unif")
text(prune.car,pretty=0,cex=0.7)
which.min(cv.car$dev)
# Recalculate test MSE
# obtain test MSE
pred2=predict(tree.car,newdata=Carseats[-train,])
car.test2=Carseats[-train,"Sales"]
(test.MSE <- mean((pred2-car.test2)^2))
# Recalculate test MSE
# obtain test MSE
pred2=predict(cv.car),newdata=Carseats[-train,])
# Recalculate test MSE
# obtain test MSE
pred2=predict(cv.car,newdata=Carseats[-train,])
# Recalculate test MSE
# obtain test MSE
pred2=predict(prune.car,newdata=Carseats[-train,])
(test.MSE <- mean((pred2-car.test2)^2))
set.seed(123)
library(dplyr)
library(tree)
library(ISLR)
attach(Carseats)
High=ifelse(Sales<=8,"No","Yes")
High=as.factor(ifelse(Sales<=8,"No","Yes"))
# split data into training set and test set
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
## Problem 2
# Fit a regression tree and plot the results
set.seed(123)
tree.car=tree(Sales~.,Carseats,subset=train)
summary(tree.car)
plot(tree.car, type = "unif")
text(tree.car,pretty=0,cex=0.7)
# obtain test MSE
pred=predict(tree.car,newdata=Carseats[-train,])
car.test=Carseats[-train,"Sales"]
(test.MSE <- mean((pred-car.test)^2))
## Problem 3
# Use cross validation to determine the optimal level of tree complexity
set.seed(123)
cv.car=cv.tree(tree.car)
cv.car
plot(cv.car)
which.min(cv.car$dev)
best_val <- (cv.car$size[13])
# Prune the tree
set.seed(123)
prune.car=prune.tree(tree.car,best=best_val)
plot(prune.car,type = "unif")
text(prune.car,pretty=0,cex=0.7)
# Recalculate test MSE
# obtain test MSE
pred2=predict(prune.car,newdata=Carseats[-train,])
(test.MSE <- mean((pred2-car.test2)^2))
# Recalculate test MSE
# obtain test MSE
pred2=predict(prune.car,newdata=Carseats[-train,])
(test.MSE <- mean((pred2-car.test)^2))
str(Boston)
attach(Boston)
str(Carsetas)
str(Carseats)
## Problem 4
# Use the bagging approach to analyze this data
bag.car=randomForest(medv~.,data=Carseats,subset=train,mtry=11,importance=TRUE)
install.packages("randomForest")
## Problem 4
library(randonForest)
# Use the bagging approach to analyze this data
bag.car=randomForest(medv~.,data=Carseats,subset=train,mtry=11,importance=TRUE)
## Problem 4
library(randomForest)
# Use the bagging approach to analyze this data
bag.car=randomForest(medv~.,data=Carseats,subset=train,mtry=11,importance=TRUE)
?randomForest
# Use the bagging approach to analyze this data
set.seed(123)
bag.car=randomForest(Sales~.,data=Carseats,subset=train,mtry=10,importance=TRUE)
bag.car
# Test MSE
pred3=predict(bag.car,newdata=Carseats[-train,])
(test.MSE <- mean((pred3-car.test)^2))
# use the importance function to determine which variables are most important
importance(bag.car)
## Problem 5
# Use random forest to analyze this data (use p/3 for regression)
rf.boston=randomForest(Sales~.,data=Carseats,subset=train,mtry=4,importance=TRUE)
## Problem 5
# Use random forest to analyze this data (use p/3 for regression)
rf.car=randomForest(Sales~.,data=Carseats,subset=train,mtry=4,importance=TRUE)
rf.car
# test MSE
pred4=predict(rf.car,newdata=Carseats[-train,])
(test.MSE <- mean((pred4-car.test)^2))
# use the importance function to determine which variables are most important
importance(rf.car)
# describe the effect of 'm', the number of variables considered at each split, on the error rate obtained
# try a couple different m values
rf.car.2=randomForest(Sales~.,data=Carseats,subset=train,mtry=2,importance=TRUE)
rf.car.4=randomForest(Sales~.,data=Carseats,subset=train,mtry=4,importance=TRUE)
rf.car.6=randomForest(Sales~.,data=Carseats,subset=train,mtry=6,importance=TRUE)
rf.car.8=randomForest(Sales~.,data=Carseats,subset=train,mtry=8,importance=TRUE)
pred.2=predict(rf.car.2,newdata=Carseats[-train,])
pred.4=predict(rf.car.4,newdata=Carseats[-train,])
pred.6=predict(rf.car.6,newdata=Carseats[-train,])
pred.8=predict(rf.car.8,newdata=Carseats[-train,])
(test.MSE <- mean((pred.2-car.test)^2))
(test.MSE <- mean((pred.4-car.test)^2))
(test.MSE <- mean((pred.6-car.test)^2))
(test.MSE <- mean((pred.8-car.test)^2))
# describe the effect of 'm', the number of variables considered at each split, on the error rate obtained
# try a couple different m values
set.seed(123)
rf.car.2=randomForest(Sales~.,data=Carseats,subset=train,mtry=2,importance=TRUE)
set.seed(123)
rf.car.4=randomForest(Sales~.,data=Carseats,subset=train,mtry=4,importance=TRUE)
set.seed(123)
rf.car.6=randomForest(Sales~.,data=Carseats,subset=train,mtry=6,importance=TRUE)
set.seed(123)
rf.car.8=randomForest(Sales~.,data=Carseats,subset=train,mtry=8,importance=TRUE)
pred.2=predict(rf.car.2,newdata=Carseats[-train,])
pred.4=predict(rf.car.4,newdata=Carseats[-train,])
pred.6=predict(rf.car.6,newdata=Carseats[-train,])
pred.8=predict(rf.car.8,newdata=Carseats[-train,])
(test.MSE <- mean((pred.2-car.test)^2))
(test.MSE <- mean((pred.4-car.test)^2))
(test.MSE <- mean((pred.6-car.test)^2))
(test.MSE <- mean((pred.8-car.test)^2))
# describe the effect of 'm', the number of variables considered at each split, on the error rate obtained
# try a couple different m values
set.seed(123)
rf.car.2=randomForest(Sales~.,data=Carseats,subset=train,mtry=2,importance=TRUE)
rf.car.4=randomForest(Sales~.,data=Carseats,subset=train,mtry=4,importance=TRUE)
rf.car.6=randomForest(Sales~.,data=Carseats,subset=train,mtry=6,importance=TRUE)
rf.car.8=randomForest(Sales~.,data=Carseats,subset=train,mtry=8,importance=TRUE)
pred.2=predict(rf.car.2,newdata=Carseats[-train,])
pred.4=predict(rf.car.4,newdata=Carseats[-train,])
pred.6=predict(rf.car.6,newdata=Carseats[-train,])
pred.8=predict(rf.car.8,newdata=Carseats[-train,])
(test.MSE <- mean((pred.2-car.test)^2))
(test.MSE <- mean((pred.4-car.test)^2))
(test.MSE <- mean((pred.6-car.test)^2))
(test.MSE <- mean((pred.8-car.test)^2))
dti <- read.csv("DTI.csv")
str(dti)
min_z = min(dti$Zscore)
mid_z = mean(dti$Zscore)
max_z = max(dti$Zscore)
library(ggplot2)
ggplot(data = dti, aes(x = x, y = y, fill = Zscore)) + geom_tile(color = "grey") +
scale_fill_gradient2(low = "blue", high = "red", midpoint = mid_z,
limit = c(min_z, max_z), space = "Lab", name = "Colour bar") +
theme_light() + coord_fixed()
setwd("~/Virginia_Tech/FA21/CS5525/final_project/CS5525-Final-Project")
install.packages("class")
install.packages("caret")
install.packages("kernlab")
set.seed(123)
X <- as.matrix(heart[, c("age", "sex", "cp", "trestbps", "chol", "fbs",
"restecg", "thalach", "exang", "oldpeak", "slope",
"ca", "thal")
]
)
X <- as.matrix(heart, c("age", "sex", "cp", "trestbps", "chol", "fbs",
"restecg", "thalach", "exang", "oldpeak", "slope",
"ca", "thal"))
setwd("~/Virginia_Tech/FA21/CS5525/final_project/CS5525-Final-Project")
#setwd(" ") # uncomment to set working directory via code
library(tree)
library(randomForest)  # bootsrap/bagging & random forest
library(class)         # KNN
library(glmnet)        # logistic regression
# Read in and organize data
## -- Read data
heart <- read.csv("heart.csv")
Target <- as.factor(heart$target) # target heart rate
## -- Split into training and test sets
train <- sample(1:nrow(heart), 0.75*nrow(heart))
heart.test <- heart[-train, ]
Target.test <- Target[-train]
# Fit a classification tree to the training data
set.seed(2441139)
tree.heart <- tree(Target~. -target, heart, subset=train)
summary(tree.heart)
## -- Plot tree
plot(tree.heart)
text(tree.heart, pretty=1, cex=0.7)
# Read in and organize data
## -- Read data
heart <- read.csv("heart.csv")
Target <- as.factor(heart$target) # target heart rate
## -- Split into training and test sets
train <- sample(1:nrow(heart), 0.75*nrow(heart))
heart.test <- heart[-train, ]
Target.test <- Target[-train]
# Fit a classification tree to the training data
set.seed(2441139)
tree.heart <- tree(Target~. -target, heart, subset=train)
# All Code
library(tree)
library(randomForest)  # bootsrap/bagging & random forest
# All Code
library(tree)
library(randomForest)  # bootsrap/bagging & random forest
heart
heart
heart
# Read in and organize data
## -- Read data
heart <- read.csv("heart.csv")
Target <- as.factor(heart$target) # target heart rate
## -- Split into training and test sets
train <- sample(1:nrow(heart), 0.75*nrow(heart))
heart.test <- heart[-train, ]
Target.test <- Target[-train]
# Fit a classification tree to the training data
set.seed(2441139)
tree.heart <- tree(Target~. -target, heart, subset=train)
summary(tree.heart)
## -- Plot tree
plot(tree.heart)
text(tree.heart, pretty=1, cex=0.7)
# All Code
library(tree)
library(randomForest)  # bootsrap/bagging & random forest
# Read in and organize data
## -- Read data
heart <- read.csv("heart.csv")
Target <- as.factor(heart$target) # target heart rate
## -- Split into training and test sets
train <- sample(1:nrow(heart), 0.75*nrow(heart))
heart.test <- heart[-train, ]
Target.test <- Target[-train]
# Fit a classification tree to the training data
set.seed(2441139)
tree.heart <- tree(Target~. -target, heart, subset=train)
summary(tree.heart)
## -- Plot tree
plot(tree.heart)
text(tree.heart, pretty=1, cex=0.7)
# Fit a classification tree to the training data
set.seed(2441139)
tree.heart <- tree(Target~. -target, heart, subset=train)
summary(tree.heart)
## -- Plot tree
plot(tree.heart)
text(tree.heart, pretty=1, cex=0.7)
# All Code
library(tree)
library(randomForest)  # bootsrap/bagging & random forest
# Read in and organize data
## -- Read data
heart <- read.csv("heart.csv")
Target <- as.factor(heart$target) # target heart rate
## -- Split into training and test sets
set.seed(2441139)
train <- sample(1:nrow(heart), 0.75*nrow(heart))
heart.test <- heart[-train, ]
Target.test <- Target[-train]
# Fit a classification tree to the training data
set.seed(2441139)
tree.heart <- tree(Target~. -target, heart, subset=train)
summary(tree.heart)
## -- Plot tree
plot(tree.heart)
text(tree.heart, pretty=1, cex=0.7)
# Fit a classification tree to the training data
set.seed(2441139)
tree.heart <- tree(Target~. -target, heart, subset=train)
summary(tree.heart)
## -- Plot tree
plot(tree.heart)
text(tree.heart, pretty=1, cex=0.7)
# Fit a classification tree to the training data
set.seed(2441139)
tree.heart <- tree(Target~. -target, heart, subset=train)
summary(tree.heart)
## -- Plot tree
plot(tree.heart)
text(tree.heart, pretty=1, cex=0.7)
# All Code
library(tree)
library(randomForest)  # bootsrap/bagging & random forest
# Read in and organize data
## -- Read data
heart <- read.csv("heart.csv")
Target <- as.factor(heart$target) # target heart rate
## -- Split into training and test sets
set.seed(2441139)
train <- sample(1:nrow(heart), 0.75*nrow(heart))
heart.test <- heart[-train, ]
Target.test <- Target[-train]
# Fit a classification tree to the training data
set.seed(2441139)
tree.heart <- tree(Target~. -target, heart, subset=train)
summary(tree.heart)
## -- Plot tree
plot(tree.heart)
text(tree.heart, pretty=1, cex=0.7)
![Original Classification Tree](setwd)
## -- Plot tree
plot(tree.heart)
text(tree.heart, pretty=1, cex=1)
## -- Plot tree
plot(tree.heart)
text(tree.heart, pretty=1, cex=1.5)
## -- Plot tree
plot(tree.heart)
text(tree.heart, pretty=1, cex=1.25)
text(tree.heart, pretty=1, cex=1)
## -- Plot tree
plot(tree.heart)
text(tree.heart, pretty=1, cex=1)
# Prune the classification tree
set.seed(2441139)
cv.heart <- cv.tree(tree.heart, FUN=prune.misclass)
cv.heart
plot(cv.heart$size, cv.heart$dev, type='b')
set.seed(2441139)
prune.heart <- prune.misclass(tree.heart, best=10)
plot(prune.heart)
text(prune.heart, pretty=1, cex=0.65)
set.seed(2441139)
prune.heart <- prune.misclass(tree.heart, best=10)
plot(prune.heart)
text(prune.heart, pretty=1, cex=0.65)
text(prune.heart, pretty=1, cex=1)
plot(prune.heart)
text(prune.heart, pretty=1, cex=1)
table(prune.pred, Target.test)
table(tree.pred, Target.test)
# Predict using test set and pruned tree. Compare.
tree.pred <- predict(tree.heart, heart.test, type='class')   # test tree
prune.pred <- predict(prune.heart, heart.test, type='class') # pruned tree
table(prune.pred, Target.test)
table(tree.pred, Target.test)
# ---------------------------------------------------------------------------- #
#                                  Bagging                                     #
# ---------------------------------------------------------------------------- #
set.seed(2441139)
# Perform bagging
bag.heart <- randomForest(as.factor(as.character(heart$target))~., data=heart,
subset=train, mtry=ncol(heart)-1,
importance=TRUE)
bag.heart
# Predict on bagged tree
bag.pred <- predict(bag.heart, heart.test, type='class')
table(bag.pred, Target.test)
varImpPlot(bag.heart)
# ---------------------------------------------------------------------------- #
#                                  Bagging                                     #
# ---------------------------------------------------------------------------- #
set.seed(2441139)
# Perform bagging
bag.heart <- randomForest(as.factor(as.character(heart$target))~., data=heart,
subset=train, mtry=ncol(heart)-1,
importance=TRUE)
bag.heart
# Predict on bagged tree
bag.pred <- predict(bag.heart, heart.test, type='class')
table(bag.pred, Target.test)
varImpPlot(bag.heart)
# ---------------------------------------------------------------------------- #
#                                 Random Forest                                #
# ---------------------------------------------------------------------------- #
set.seed(2441139)
# Perform Random Forest
rf.heart <- randomForest(as.factor(as.character(heart$target))~., data=heart,
subset=train, mtry=sqrt(ncol(heart)-1),
ntree=25, importance=TRUE)
rf.heart
# Predict on the forest
rf.pred <- predict(rf.heart, heart.test, type='class')
table(rf.pred, Target.test)
varImpPlot(rf.heart)
varImpPlot(rf.heart)
# Perform Random Forest
rf.heart <- randomForest(as.factor(as.character(heart$target))~., data=heart,
subset=train, mtry=sqrt(ncol(heart)-1),
ntree=25, importance=TRUE)
rf.heart
# Predict on the forest
rf.pred <- predict(rf.heart, heart.test, type='class')
table(rf.pred, Target.test)
varImpPlot(rf.heart)
# ---------------------------------------------------------------------------- #
#                                 Random Forest                                #
# ---------------------------------------------------------------------------- #
set.seed(2441139)
# Perform Random Forest
rf.heart <- randomForest(as.factor(as.character(heart$target))~., data=heart,
subset=train, mtry=sqrt(ncol(heart)-1),
ntree=25, importance=TRUE)
rf.heart
# Predict on the forest
rf.pred <- predict(rf.heart, heart.test, type='class')
table(rf.pred, Target.test)
varImpPlot(rf.heart)
# ---------------------------------------------------------------------------- #
# ---------------------------------------------------------------------------- #
#                      Determine Best Model (Random Forest)                    #
# ---------------------------------------------------------------------------- #
# ---------------------------------------------------------------------------- #
# Investigate how mtry affect the accuracy
Acc <- rep(0,ncol(heart)-2)
for (m in 1:(ncol(heart)-2)){
set.seed(2441139)
rf.heart <- randomForest(as.factor(as.character(heart$target))~., data=heart,
subset=train, mtry=m,
ntree=25)
rf.pred <- predict(rf.heart, heart.test, type='class')
t <- table(rf.pred, Target.test)
acc <- sum(diag(t))/sum(t)
Acc[m] <- acc
}
mbest <- which(Acc==max(Acc))
mbest
plot(1:(ncol(heart)-2), Acc, xlab='mtry', ylab='Accuracy of random forest') # include plot in final submission report
# Now use the best value of m for the random forest
set.seed(2441139)
rf.heart <- randomForest(as.factor(as.character(heart$target))~., data=heart,
subset=train, mtry=mbest,
ntree=25, importance=TRUE)
rf.heart
# Predict on the forest
rf.pred <- predict(rf.heart, heart.test, type='class')
table(rf.pred, Target.test)
varImpPlot(rf.heart)
plot(1:(ncol(heart)-2), Acc, xlab='mtry', ylab='Accuracy of random forest') # include plot in final submission report
