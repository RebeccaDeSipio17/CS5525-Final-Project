---
title: "CS5525 Project"
author: "Jennifer Appiah-Kubi, Rebecca DeSipio, Ajinkya Fotedar"
date: "11/30/2021"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
---

# Introduction

- In this project, we will be trying to predict the probability of having a heart attack using 14 variables available in the `hearts.csv` data-set. 

- Techniques employed for model fit, analysis and interpretation, and visualization:
    
    - Classification
    
        1. Logistic Regression
        2. Decision trees
        3. Support Vector Machines
        
    - Sparse Regression
    
        1. LASSO
        2. Elastic Net 

- Libraries used:
  
    1. `glmnet`
    2. `tree`
    3. `randomForest`

## Data-set

```{r}
# reading data
setwd("/Users/ajinkyafotedar/CS5525/Project/CS5525-Final-Project")
heart <- read.csv("heart.csv")

# observations
dim(heart)

# attributes
names(heart)
```

## Attribute Information

- age
- sex
- chest pain type (4 values)
- resting blood pressure
- serum cholesterol in mg/dl
- fasting blood sugar > 120 mg/dl
- resting electrocardiograph results (values 0, 1, 2)
- maximum heart rate achieved
- exercise induced angina
- old peak = ST depression induced by exercise relative to rest
- the slope of the peak exercise ST segment
- number of major vessels (0 - 3) colored by fluoroscope
- thal: 0 = normal; 1 = fixed defect; 2 = reversible defect
- target: 0 = less chance of heart attack; 1 = more chance of heart attack

## Splitting Into Train and Test

```{r message=FALSE, warning=FALSE}
set.seed(123)
grid = 10^seq(10, -2, length = 100)

X <- data.matrix(heart[, c("age", "sex", "cp", "trestbps", "chol", "fbs", 
                           "restecg", "thalach", "exang", "oldpeak", "slope",
                           "ca", "thal")
                       ]
                 )
y <- heart$target

n = nrow(X)
train_rows <- sample(1:n, n * 0.7)

X.train <- X[train_rows,]
X.test <- X[-train_rows,]
y.train <- y[train_rows]
y.test <- y[-train_rows]

dim(X.train)
dim(X.test)
```

# Sparse Regression Methods

## Lasso

```{r message=FALSE, warning=FALSE}
library(glmnet)

# lasso model
lasso.mod <- glmnet(X.train, y.train, alpha = 1, lambda = grid)
plot(lasso.mod, xvar = "lambda", label = T)

# cross-validation for lambda
cv.out <- cv.glmnet(X.train, y.train, alpha = 1)
bestlam <- cv.out$lambda.min

# test error
lasso.pred <- predict(lasso.mod, s = bestlam, newx = X.test)
lasso.mse <- mean((lasso.pred - y.test)^2)
lasso.mse

# non-zero coefficients
lasso.coef <- predict(lasso.mod, type = "coefficients", s = bestlam)
lasso.coef <- lasso.coef[which(lasso.coef != 0)]
lasso.coef

# coefficients of the best model
best.lasso.mod <- glmnet(X.train, y.train, alpha = 1, lambda = bestlam)
coef(best.lasso.mod)
```

## Elastic Net

```{r message=FALSE, warning=FALSE}
# elastic net model
en.mod <- glmnet(X.train, y.train, alpha = 0.5, lambda = grid)
plot(en.mod, xvar = "lambda", label = T)

# cross-validation for lambda (with a fixed alpha)
cv.out <- cv.glmnet(X.train, y.train, alpha = 0.5)
bestlam <- cv.out$lambda.min

# test error
en.pred <- predict(en.mod, s = bestlam, newx = X.test)
en.mse <- mean((en.pred - y.test)^2)
en.mse

# non-zero coefficients
en.coef <- predict(en.mod, type = "coefficients", s = bestlam)
en.coef <- en.coef[which(en.coef != 0)]
en.coef

# coefficients of the best model
best.en.mod <- glmnet(X.train, y.train, alpha = 0.5, lambda = bestlam)
coef(best.en.mod)
```

# Classification Methods

## Logistic Regression

```{r}
# splitting data
target <- as.factor(heart$target)
train <- sample(1:nrow(heart), 0.75 * nrow(heart))

heart.train <- heart[train, ]
heart.test <- heart[-train, ]

# training data
dim(heart.train)

# testing data
dim(heart.test)
```

## Decision Trees

## Support Vector Machines

# Analysis

# Conclusion
