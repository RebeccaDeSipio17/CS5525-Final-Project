---
title: "CS5525 Final Project Code Submission"
author: "Jennifer Appiah-Kubi, Rebecca DeSipio, Ajinkya Fotedar"
date: "12/11/2021"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

# Classification Methods

First set the directory (path which contains the `heart.csv` data), and import any needed libraries

```{r}
#setwd(" ") # uncomment to set working directory via code
library(tree)
library(randomForest)  # bootsrap/bagging & random forest
library(class)         # KNN
library(caret)         # SVM
library(ggplot2)
library(cowplot)
```

## Decision Trees

```{r}

# Read in and organize data
## -- Read data
heart <- read.csv("heart.csv")
Target <- as.factor(heart$target) # target heart rate

## -- Split into training and test sets
train <- sample(1:nrow(heart), 0.75*nrow(heart))
heart.test <- heart[-train, ]
Target.test <- Target[-train]

# ---------------------------------------------------------------------------- #
#                        Fit a Classification Tree                             #
# ---------------------------------------------------------------------------- #

# Fit a classification tree to the training data
set.seed(2441139)
tree.heart <- tree(Target~. -target, heart, subset=train)
summary(tree.heart)

## -- Plot tree
plot(tree.heart)
text(tree.heart, pretty=1, cex=0.7)


# Prune the classification tree
set.seed(2441139)
cv.heart <- cv.tree(tree.heart, FUN=prune.misclass)
cv.heart
plot(cv.heart$size, cv.heart$dev, type='b')
prune.heart <- prune.misclass(tree.heart, best=10)
plot(prune.heart)
text(prune.heart, pretty=1, cex=0.65)

# Predict using test set and pruned tree. Compare.
tree.pred <- predict(tree.heart, heart.test, type='class')   # test tree
prune.pred <- predict(prune.heart, heart.test, type='class') # pruned tree

table(prune.pred, Target.test)
table(tree.pred, Target.test)

# ---------------------------------------------------------------------------- #
#                                  Bagging                                     #
# ---------------------------------------------------------------------------- #
set.seed(2441139)

# Perform bagging
bag.heart <- randomForest(as.factor(as.character(heart$target))~., data=heart,
                          subset=train, mtry=ncol(heart)-1,
                          importance=TRUE)
bag.heart

# Predict on bagged tree
bag.pred <- predict(bag.heart, heart.test, type='class')
table(bag.pred, Target.test)
varImpPlot(bag.heart)

# ---------------------------------------------------------------------------- #
#                                 Random Forest                                #
# ---------------------------------------------------------------------------- #
set.seed(2441139)

# Perform Random Forest
rf.heart <- randomForest(as.factor(as.character(heart$target))~., data=heart,
                          subset=train, mtry=sqrt(ncol(heart)-1),
                          ntree=25, importance=TRUE)
rf.heart

# Predict on the forest
rf.pred <- predict(rf.heart, heart.test, type='class')
table(rf.pred, Target.test)
varImpPlot(rf.heart)


# ---------------------------------------------------------------------------- #
# ---------------------------------------------------------------------------- #
#                      Determine Best Model (Random Forest)                    #
# ---------------------------------------------------------------------------- #
# ---------------------------------------------------------------------------- #
# Investigate how mtry affect the accuracy
Acc <- rep(0,ncol(heart)-2)
for (m in 1:(ncol(heart)-2)){
  set.seed(2441139)
  rf.heart <- randomForest(as.factor(as.character(heart$target))~., data=heart,
                           subset=train, mtry=m,
                           ntree=25)
  rf.pred <- predict(rf.heart, heart.test, type='class')
  t <- table(rf.pred, Target.test)
  acc <- sum(diag(t))/sum(t)
  Acc[m] <- acc
}
mbest <- which(Acc==max(Acc))
plot(1:(ncol(heart)-2), Acc, xlab='mtry', ylab='Accuracy of random forest') # include plot in final submission report


# Now use the best value of m for the random forest
set.seed(2441139)
rf.heart <- randomForest(as.factor(as.character(heart$target))~., data=heart,
                         subset=train, mtry=mbest,
                         ntree=25, importance=TRUE)
rf.heart

# Predict on the forest
rf.pred <- predict(rf.heart, heart.test, type='class')
table(rf.pred, Target.test)
varImpPlot(rf.heart)
```

## KNN

```{r}
# K-Nearest Neighbor
set.seed(2441139)

# Read data
heart <- read.csv("heart.csv")
Target <- as.factor(heart$target)

# Split into training and test sets
train <- sample(1:nrow(heart), 0.75*nrow(heart))
heart.test <- heart[-train, ]
Target.test <- Target[-train]

# Normalizing function
normalizer <-function(x){(x -min(x))/(max(x)-min(x))}

# Use normalized set to do cross-validation
trControl <- trainControl(method  = "cv",
                          number  = 5)
fit <- train(as.factor(target) ~ .,
             method     = "knn",
             tuneGrid   = expand.grid(k = 1:15),
             trControl  = trControl,
             metric     = "Accuracy",
             data       = as.data.frame(lapply(heart, normalizer)))

# For best value of k perform training
heart.norm <- as.data.frame(lapply(heart[,1:13], normalizer))

# Get training and test set
heart.norm.train <- heart.norm[train,]
heart.norm.test <- heart.norm[-train,]

# Get cl argument
cl <- as.factor(heart$target[train])

# Train and get confusion matrix
knn.heart <- knn(heart.norm.train, heart.norm.test, cl, k = 12, prob=TRUE)
cm <- table(knn.heart, Target.test)
(cm)
acc <- sum(diag(cm))/sum(cm)

# Train and get confusion matrix for un-normalized data
knn.heart <- knn(heart[train,1:13],heart[-train,1:13], cl, k = 5, prob=TRUE)
cm <- table(knn.heart, Target.test)
(cm)
acc <- sum(diag(cm))/sum(cm)
```

## SVM

```{r}
set.seed(2441139)

# splitting data into test and train
intrain <- createDataPartition(y = heart$target, p = 0.7, list = F)

training <- heart[intrain,]
testing <- heart[-intrain,]
training[["target"]] <- as.factor(training[["target"]])

dim(training)
dim(testing)

# model training with svm
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
svm.mod <- train(target ~ ., data = training, method = "svmLinear",
                 trControl = trctrl,
                 preProcess = c("center", "scale"),
                 tuneLength = 10)
svm.mod

# predction using the above model
svm.pred <- predict(svm.mod, newdata = testing)
svm.pred

# accuracy of the trained model
confusionMatrix(table(svm.pred, testing$target))

# costs for further tuning with 10-fold cross-validation
grid <- expand.grid(C = c(0, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 5))

svm.mod.grid <- train(target ~ ., data = training, method = "svmLinear",
                      trControl = trctrl,
                      preProcess = c("center", "scale"),
                      tuneGrid = grid,
                      tuneLength = 10)
svm.mod.grid

# accuracy plot of tuned model
plot(svm.mod.grid)

# prediction using tuned model
svm.pred.grid <- predict(svm.mod.grid, newdata = testing)
svm.pred.grid

# accuracy of the tuned model
confusionMatrix(table(svm.pred.grid, testing$target))
```

## Logistic Regression

```{r}
set.seed(2441139)

# pre-processing of data
str(heart)

heart$sex <- ifelse(test = heart$sex == 0, yes = "F", no = "M")
heart$sex <- as.factor(heart$sex)
heart$cp <- as.factor(heart$cp)
heart$fbs <- as.factor(heart$fbs)
heart$restecg <- as.factor(heart$restecg)
heart$exang <- as.factor(heart$exang)
heart$slope <- as.factor(heart$slope)
heart$ca <- as.factor(heart$ca)
heart$thal <- as.factor(heart$thal)
heart$age <- as.numeric(heart$age)
heart$trestbps <- as.numeric(heart$trestbps)
heart$chol <- as.numeric(heart$chol)
heart$thalach <- as.numeric(heart$thalach)
heart$target <- ifelse(test = heart$target == 0, yes = "Healthy", no = "Unhealthy")
heart$target <- as.factor(heart$target)

str(heart)

# getting the number of samples based on gender
xtabs(~ target + sex, data = heart)

# simple logistic model
logistic <- glm(target ~ sex, data = heart, family = "binomial")
summary(logistic)

R_sq_1 <- 1 - logistic$deviance / logistic$null.deviance
R_sq_1
BIC_1 <- logistic$deviance + 2 * log(dim(heart)[1])
BIC_1

# complex logistic model
logistic <- glm(target ~ ., data = heart, family = "binomial")
summary(logistic)

R_sq_2 <- 1 - logistic$deviance / logistic$null.deviance
R_sq_2
BIC_2 <- logistic$deviance + 14 * log(dim(heart)[1])
BIC_2

# why age isn't of statistical significance
median(heart$age)

# plotting the probability of getting a heart disease
predict.hd <- data.frame(prob.of.hd = logistic$fitted.values, hd = heart$target)
predict.hd <- predict.hd[order(predict.hd$prob.of.hd, decreasing = FALSE), ]
predict.hd$rank <- 1:nrow(predict.hd)

ggplot(data = predict.hd, aes(x = rank, y = prob.of.hd)) +
  geom_point(aes(color = hd), alpha = 1, shape = 4, stroke = 2) +
  xlab("Index") +
  ylab("Predicted probability of getting a heart disease")

# predicting model accuracy
log.mod <- glm(target ~ ., data = training, family = "binomial")
log.pred <- predict(log.mod, newdata = testing, type = "response")
log.pred <- ifelse(log.pred > 0.5, 1, 0)
confusionMatrix(table(log.pred, testing$target))
```
